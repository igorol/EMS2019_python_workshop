{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import regionmask\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Renewable Energy data\n",
    "\n",
    "- **Source:** https://data.open-power-system-data.org/time_series/2019-06-05\n",
    "\n",
    "\n",
    "- **Description:** This data package contains different kinds of timeseries data relevant for power system modelling, namely electricity consumption (load) for 37 European countries as well as wind and solar power generation and capacities and prices for a growing subset of countries. The timeseries become available at different points in time depending on the sources. The data has been downloaded from the sources, resampled and merged in a large CSV file with hourly resolution. Additionally, the data available at a higher resolution (Some renewables in-feed, 15 minutes) is provided in a separate file. All data processing is conducted in python and pandas and has been documented in the Jupyter notebooks linked below.\n",
    "\n",
    "\n",
    "#### This dataset is a consolidation from several different sources. Due to different quality standards for different countries, for the example in the current notebook let's use data for Germany (country code = 'DE_')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# energy data file\n",
    "energy_fn = './data/energy/time_series_60min_singleindex.csv.gz'\n",
    "\n",
    "# country to be extracted from the energy dataset\n",
    "country_code = 'DE_'\n",
    "\n",
    "# function to extract only the needed columns\n",
    "usecols = lambda colname: colname.startswith('utc') | colname.startswith(country_code)\n",
    "\n",
    "# reading CSV into a pd.DataFrame, selecting columns with 'usecols'\n",
    "energy_df = pd.read_csv(energy_fn,\n",
    "                        usecols=usecols,          \n",
    "                        parse_dates=['utc_timestamp'],\n",
    "                        index_col=['utc_timestamp'])\n",
    "\n",
    "# slicing a period of the data\n",
    "energy_df = energy_df['2016-01-01 00:00:00':'2018-12-31 23:00:00']\n",
    "energy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplots are slow when many vertical bars have to be drawn, be patient (or change to lineplot)\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.bar(energy_df.index, energy_df[f'{country_code}solar_generation_actual'], color='#f01212')\n",
    "## use this line if impatient\n",
    "# plt.plot(energy_df.index, energy_df.NL_solar_generation_actual)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title(f'Actual solar generation for {country_code[:-1]}')\n",
    "plt.ylabel('MW')\n",
    "plt.xlabel('hourly data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barplots are slow when many vertical bars have to be drawn, be patient (or change to lineplot)\n",
    "plt.figure(figsize=(18,8))\n",
    "plt.bar(energy_df.index, energy_df[f'{country_code}wind_offshore_generation_actual'], color='#196DAE')\n",
    "## use this line if impatient\n",
    "# plt.plot(energy_df.index, energy_df.NL_wind_offshore_generation_actual)\n",
    "plt.xticks(rotation=60)\n",
    "plt.title(f'Actual wind generation for {country_code[:-1]}')\n",
    "plt.ylabel('MW')\n",
    "plt.xlabel('hourly data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Meteorological data \n",
    "\n",
    "- Let's use pre-downloaded hourly data, link provided in README\n",
    "- Hourly data gives us the same time granularity as in the energy data. We'll slice the data accordingly so both datasets cover the exact same period (and have the same shape)\n",
    "- Meteorological features for prediction are:\n",
    "\n",
    "|Variable|Variable Name|\n",
    "|-------:|------------:|\n",
    "|t2m|2 metre temperature|\n",
    "|msl|Mean sea level pressure|\n",
    "|u10|10 metre U wind component|\n",
    "|v10|10 metre V wind component|\n",
    "|u100|100 metre U wind component|\n",
    "|v100|100 metre V wind component|\n",
    "|fsr|Forecast surface roughness|\n",
    "|cdir|Clear-sky direct solar radiation at surface|\n",
    "|ssrdc|Surface solar radiation downward clear-sky|\n",
    "|ssrd|Surface solar radiation downwards|\n",
    "|tisr|TOA incident solar radiation|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wx data for each year\n",
    "wx_data_2016 = xr.open_dataset('./data/weather/era5_DE_hourly_2016.nc')\n",
    "wx_data_2017 = xr.open_dataset('./data/weather/era5_DE_hourly_2017.nc')\n",
    "wx_data_2018 = xr.open_dataset('./data/weather/era5_DE_hourly_2018.nc')\n",
    "\n",
    "# combining wx data, cleaning unused object for memory efficiency\n",
    "wx_data = xr.concat([wx_data_2016, wx_data_2017, wx_data_2018], dim='time')\n",
    "del wx_data_2016, wx_data_2017, wx_data_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's re-use the extract function from previous example\n",
    "country_mask = regionmask.defined_regions.natural_earth.countries_50.mask(wx_data, \n",
    "                                                                          lon_name='longitude', \n",
    "                                                                          lat_name='latitude')\n",
    "\n",
    "def extract_data_for_country(country_name, country_mask, wx_data):\n",
    "    country_id = regionmask.defined_regions.natural_earth.countries_50.map_keys(country_name)\n",
    "    wx_data = wx_data.where(country_mask==country_id)\n",
    "    wx_data = wx_data.dropna('latitude', how='all')\n",
    "    wx_data = wx_data.dropna('longitude', how='all')\n",
    "    return wx_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wx_data = extract_data_for_country('Germany', country_mask, wx_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "data_crs = ccrs.PlateCarree()\n",
    "\n",
    "plt.figure(figsize=(13,13))\n",
    "data_crs = ccrs.PlateCarree()\n",
    "\n",
    "ax = plt.axes(projection=data_crs)\n",
    "wx_data.t2m.sel(time='2016-06-01 00:00:00').plot(ax=ax,transform=data_crs, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3a: Modeling for Wind Energy\n",
    "\n",
    "- How do the same points from the previous example work in a neural network case?\n",
    "\n",
    "## Important considerations:\n",
    "    - Training/*Validation*/Test Split\n",
    "    - Features to be used\n",
    "    - Algorithm to choose\n",
    "        - # layers\n",
    "        - # units\n",
    "        - Neural networks hyperparameters\n",
    "    - Always check your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_period = ('2016-01-01 00:00:00','2017-12-31 23:00:00')\n",
    "valid_period = ('2018-01-01 00:00:00','2018-06-30 23:00:00')\n",
    "test_period  = ('2018-07-01 00:00:00','2018-12-31 23:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing train, valid and test data\n",
    "features = ['t2m', 'msl', 'u10', 'v10', 'u100', 'v100']\n",
    "\n",
    "X_train = wx_data[features].sel(time=slice(train_period[0],train_period[1])).mean(axis=(1,2)).to_dataframe()\n",
    "X_valid = wx_data[features].sel(time=slice(valid_period[0],valid_period[1])).mean(axis=(1,2)).to_dataframe()\n",
    "X_test = wx_data[features].sel(time=slice(test_period[0],test_period[1])).mean(axis=(1,2)).to_dataframe()\n",
    "\n",
    "# saving column names for later\n",
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we scale the data before ingesting in NNs?\n",
    "![](./imgs/scaling.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling X data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.values)\n",
    "X_valid = scaler.transform(X_valid.values)\n",
    "X_test = scaler.transform(X_test.values)\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train, columns=cols)\n",
    "X_train.index.names = ['time']\n",
    "\n",
    "X_valid = pd.DataFrame(data=X_valid, columns=cols)\n",
    "X_valid.index.names = ['time']\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test, columns=cols)\n",
    "X_test.index.names = ['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape, X_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing Train, Test and Validation Sets\n",
    "labels = [f'{country_code}wind_onshore_generation_actual']\n",
    "Y_train = energy_df[labels][train_period[0]:train_period[1]]\n",
    "Y_valid = energy_df[labels][valid_period[0]:valid_period[1]]\n",
    "Y_test = energy_df[labels][test_period[0]:test_period[1]]\n",
    "\n",
    "Y_train.shape, Y_valid.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering some NaN's present in labels\n",
    "\n",
    "X_train = X_train[Y_train.notnull().values]\n",
    "Y_train = Y_train[Y_train.notnull().values]\n",
    "\n",
    "X_valid = X_valid[Y_valid.notnull().values]\n",
    "Y_valid = Y_valid[Y_valid.notnull().values]\n",
    "\n",
    "X_test = X_test[Y_test.notnull().values]\n",
    "Y_test = Y_test[Y_test.notnull().values]\n",
    "\n",
    "X_train.shape, X_valid.shape, X_test.shape,Y_train.shape, Y_valid.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some comments on Keras module\n",
    "\n",
    "\n",
    "- Documentation: https://www.tensorflow.org/api_docs/python/tf/keras\n",
    "\n",
    "\n",
    "#### Activation functions: https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "- Let's use the 'rectified linear unit' (relu) activation function\n",
    "\n",
    "#### Optimizers:\n",
    "- Let's use the Adam optmizer: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "    - Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. According to the paper [Adam: A Method for Stochastic Optimization. Kingma et al., 2014,](http://arxiv.org/abs/1412.6980) the method is \"computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters\".\n",
    "    \n",
    "- we won't change the default parameters of the Adam optmizer:\n",
    "```\n",
    "    __init__(\n",
    "        learning_rate=0.001,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name='Adam',\n",
    "        **kwargs\n",
    "        )\n",
    "```\n",
    "\n",
    "#### Initializers: https://www.tensorflow.org/api_docs/python/tf/keras/initializers\n",
    "- Let's use a RandomNormal() initalizer\n",
    "\n",
    "\n",
    "### Why do we have to define `Checkpoint` and `Earlystop`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.keras.activations.relu\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "initializer = tf.keras.initializers.RandomNormal(dtype=tf.dtypes.float32)\n",
    "loss = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "num_hidden_layers = 10\n",
    "num_units = 50\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(num_units, \n",
    "                                kernel_initializer=initializer,\n",
    "                                input_dim=X_train.shape[1]))\n",
    "\n",
    "for layers in range(num_hidden_layers):\n",
    "    model.add(tf.keras.layers.Dense(num_units, \n",
    "                                    kernel_initializer=initializer,\n",
    "                                    activation=activation))\n",
    "    \n",
    "model.add(tf.keras.layers.Dense(1, \n",
    "                                kernel_initializer=initializer,\n",
    "                                activation=activation))\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[loss])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model_wind.h5',\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                verbose=0,\n",
    "                                                save_weights_only=True,\n",
    "                                                mode=\"auto\",\n",
    "                                                save_freq=1)\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                             min_delta=0, \n",
    "                                             patience=10, \n",
    "                                             verbose=0, \n",
    "                                             mode=\"auto\")\n",
    "\n",
    "history = model.fit(X_train.values, \n",
    "                    Y_train.values.reshape(-1),\n",
    "                    validation_data=(X_valid, Y_valid),\n",
    "                    epochs = 100, \n",
    "                    batch_size=512,\n",
    "                    callbacks=[checkpoint, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before testing the model, let's analyze what happened during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history.history.keys()\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training')\n",
    "plt.xlabel('epoch number')\n",
    "\n",
    "plt.axvline(np.argmin(history.history['val_loss']), \n",
    "            linestyle='--',\n",
    "            linewidth=1)\n",
    "\n",
    "plt.axhline(np.min(history.history['val_loss']), \n",
    "            linestyle='--',\n",
    "            linewidth=1,\n",
    "            label='best model')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok now let's predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_wind.h5')\n",
    "Y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = metrics.mean_absolute_error(Y_test, Y_pred)\n",
    "mse = metrics.mean_squared_error(Y_test, Y_pred)\n",
    "msle = metrics.mean_squared_log_error(Y_test, Y_pred)\n",
    "r2 = metrics.r2_score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean Absolute Error - MAE = {mae}\")\n",
    "print(f\"Mean Squared Error - MSE = {mse}\")\n",
    "print(f\"Mean Squared Log Error - MSLE = {msle}\")\n",
    "print(f\"R**2 Score - R2 = {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(Y_test.values, label='Observations')\n",
    "plt.plot(Y_pred, label='Prediction')\n",
    "plt.legend()\n",
    "plt.title('Model Performance for Wind Energy Generation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(Y_test, Y_pred)\n",
    "l_min, l_max = -100,  np.max([np.max(Y_test.values),np.max(Y_pred)])+200\n",
    "plt.xlim([l_min, l_max])\n",
    "plt.ylim([l_min, l_max])\n",
    "plt.plot([l_min, l_max],[l_min, l_max], color='k', lw=1)\n",
    "plt.title('Scatterplot - Wind Model Performance')\n",
    "plt.xlabel('Observsations')\n",
    "plt.ylabel('Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3b: Modeling for Solar Energy\n",
    "\n",
    "- Let's use different meteorological features now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slicing train, valid and test data\n",
    "features = ['t2m', 'cdir', 'ssrdc', 'ssrd', 'tisr', 'msl'] \n",
    "\n",
    "X_train = wx_data[features].sel(time=slice(train_period[0],train_period[1])).mean(axis=(1,2)).to_dataframe()\n",
    "X_valid = wx_data[features].sel(time=slice(valid_period[0],valid_period[1])).mean(axis=(1,2)).to_dataframe()\n",
    "X_test = wx_data[features].sel(time=slice(test_period[0],test_period[1])).mean(axis=(1,2)).to_dataframe()\n",
    "\n",
    "# saving column names for later\n",
    "cols = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling X data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.values)\n",
    "X_valid = scaler.transform(X_valid.values)\n",
    "X_test = scaler.transform(X_test.values)\n",
    "\n",
    "X_train = pd.DataFrame(data=X_train, columns=cols)\n",
    "X_train.index.names = ['time']\n",
    "\n",
    "X_valid = pd.DataFrame(data=X_valid, columns=cols)\n",
    "X_valid.index.names = ['time']\n",
    "\n",
    "X_test = pd.DataFrame(data=X_test, columns=cols)\n",
    "X_test.index.names = ['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing Train, Test and Validation Sets\n",
    "labels = [f'{country_code}solar_generation_actual']\n",
    "Y_train = energy_df[labels][train_period[0]:train_period[1]]\n",
    "Y_valid = energy_df[labels][valid_period[0]:valid_period[1]]\n",
    "Y_test = energy_df[labels][test_period[0]:test_period[1]]\n",
    "\n",
    "Y_train.shape, Y_valid.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering some NaN's present in labels\n",
    "\n",
    "X_train = X_train[Y_train.notnull().values]\n",
    "Y_train = Y_train[Y_train.notnull().values]\n",
    "\n",
    "X_valid = X_valid[Y_valid.notnull().values]\n",
    "Y_valid = Y_valid[Y_valid.notnull().values]\n",
    "\n",
    "X_test = X_test[Y_test.notnull().values]\n",
    "Y_test = Y_test[Y_test.notnull().values]\n",
    "\n",
    "X_train.shape, X_valid.shape, X_test.shape,Y_train.shape, Y_valid.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "activation = tf.keras.activations.relu\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "initializer = tf.keras.initializers.RandomNormal(dtype=tf.dtypes.float32)\n",
    "loss = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "num_hidden_layers = 10\n",
    "num_units = 80\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(num_units, \n",
    "                                kernel_initializer=initializer,\n",
    "                                input_dim=X_train.shape[1]))\n",
    "\n",
    "for layers in range(num_hidden_layers):\n",
    "    model.add(tf.keras.layers.Dense(num_units, \n",
    "                                    kernel_initializer=initializer,\n",
    "                                    activation=activation))\n",
    "    \n",
    "model.add(tf.keras.layers.Dense(1, \n",
    "                                kernel_initializer=initializer,\n",
    "                                activation=activation))\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer=optimizer,\n",
    "              metrics=[loss])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model_solar.h5',\n",
    "                                                monitor=\"val_loss\",\n",
    "                                                verbose=0,\n",
    "                                                save_weights_only=True,\n",
    "                                                mode=\"auto\",\n",
    "                                                save_freq=1)\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", \n",
    "                                             min_delta=0, \n",
    "                                             patience=10, \n",
    "                                             verbose=0, \n",
    "                                             mode=\"auto\")\n",
    "\n",
    "history = model.fit(X_train.values, \n",
    "                    Y_train.values.reshape(-1),\n",
    "                    validation_data=(X_valid, Y_valid),\n",
    "                    epochs = 100, \n",
    "                    batch_size=512,\n",
    "                    callbacks=[checkpoint, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model_solar.h5')\n",
    "Y_pred = model.predict(X_test)\n",
    "mae = metrics.mean_absolute_error(Y_test, Y_pred)\n",
    "mse = metrics.mean_squared_error(Y_test, Y_pred)\n",
    "msle = metrics.mean_squared_log_error(Y_test, Y_pred)\n",
    "r2 = metrics.r2_score(Y_test, Y_pred)\n",
    "print(f\"Mean Absolute Error - MAE = {mae}\")\n",
    "print(f\"Mean Squared Error - MSE = {mse}\")\n",
    "print(f\"Mean Squared Log Error - MSLE = {msle}\")\n",
    "print(f\"R**2 Score - R2 = {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(Y_test.values, label='Observations')\n",
    "plt.plot(Y_pred, label='Prediction', alpha=0.65)\n",
    "plt.legend()\n",
    "plt.title('Model Performance for Solar Energy Generation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(Y_test, Y_pred)\n",
    "l_min, l_max = -600,  np.max([np.max(Y_test.values),np.max(Y_pred)])+200\n",
    "plt.xlim(l_min, l_max)\n",
    "plt.ylim(l_min, l_max)\n",
    "plt.plot([l_min, l_max],[l_min, l_max], color='k', lw=1)\n",
    "plt.title('Scatterplot - Solar Model Performance')\n",
    "plt.xlabel('Observsations')\n",
    "plt.ylabel('Predictions')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
